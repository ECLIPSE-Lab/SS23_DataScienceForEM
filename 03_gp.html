<!DOCTYPE html>
<html lang="en"><head>
<script src="03_gp_files/libs/clipboard/clipboard.min.js"></script>
<script src="03_gp_files/libs/quarto-html/tabby.min.js"></script>
<script src="03_gp_files/libs/quarto-html/popper.min.js"></script>
<script src="03_gp_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="03_gp_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="03_gp_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="03_gp_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="03_gp_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.361">

  <title>gp</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="03_gp_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="03_gp_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #97947a;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #97947a;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #2b2b2b; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #dcc6e0; } /* Alert */
    code span.an { color: #d4d0ab; } /* Annotation */
    code span.at { color: #ffd700; } /* Attribute */
    code span.bn { color: #dcc6e0; } /* BaseN */
    code span.bu { color: #f5ab35; } /* BuiltIn */
    code span.cf { color: #ffa07a; } /* ControlFlow */
    code span.ch { color: #abe338; } /* Char */
    code span.cn { color: #ffa07a; } /* Constant */
    code span.co { color: #d4d0ab; } /* Comment */
    code span.cv { color: #d4d0ab; font-style: italic; } /* CommentVar */
    code span.do { color: #d4d0ab; font-style: italic; } /* Documentation */
    code span.dt { color: #dcc6e0; } /* DataType */
    code span.dv { color: #dcc6e0; } /* DecVal */
    code span.er { color: #dcc6e0; } /* Error */
    code span.ex { color: #ffd700; } /* Extension */
    code span.fl { color: #f5ab35; } /* Float */
    code span.fu { color: #ffd700; } /* Function */
    code span.im { color: #f8f8f2; } /* Import */
    code span.in { color: #d4d0ab; } /* Information */
    code span.kw { color: #ffa07a; } /* Keyword */
    code span.op { color: #00e0e0; } /* Operator */
    code span.ot { color: #ffa07a; } /* Other */
    code span.pp { color: #dcc6e0; } /* Preprocessor */
    code span.sc { color: #00e0e0; } /* SpecialChar */
    code span.ss { color: #abe338; } /* SpecialString */
    code span.st { color: #abe338; } /* String */
    code span.va { color: #f5ab35; } /* Variable */
    code span.vs { color: #abe338; } /* VerbatimString */
    code span.wa { color: #d4d0ab; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="03_gp_files/libs/revealjs/dist/theme/quarto.css">
  <link href="03_gp_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="03_gp_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="03_gp_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="03_gp_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <meta name="twitter:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta name="twitter:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta name="twitter:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta name="twitter:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@thomas_mock">
  <meta name="twitter:site" content="@thomas_mock">
  <meta property="og:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta property="og:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta property="og:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta property="og:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta property="og:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta property="og:type" content="website">
  <meta property="og:locale" content="en_US">
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<h1>
Gaussian Process Inference
</h1>
<h2>
Data Science in Electron Microscopy
</h2>
<hr>
<h3>
Philipp Pelz
</h3>
<h3>
2022-06-03
</h3>
<p><br></p>
<h3>
<p>&nbsp; <a href="https://github.com/ECLIPSE-Lab/SS23_DataScienceForEM">https://github.com/ECLIPSE-Lab/SS23_DataScienceForEM</a></p>
</h3></section>
<section>
<section id="gaussian-process-inference" class="title-slide slide level1">
<h1>Gaussian Process Inference</h1>
<ul>
<li>this section: show how to perform posterior inference and make predictions using the GP priors</li>
</ul>
<div class="fragment">
<ul>
<li>We will start with regression, where we can perform <strong>inference in <em>closed form</em></strong>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>start coding all the basic operations from scratch</p></li>
<li><p>then introduce <a href="https://gpytorch.ai/">GPyTorch</a> –&gt; convenient SOTA GPs.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>consider more advanced topics in depth in the next section</p></li>
<li><p>settings where <strong>approximate inference</strong> is required — classification, point processes, or any non-Gaussian likelihoods.</p></li>
</ul>
</div>
</section>
<section id="posterior-inference-for-regression-1" class="slide level2">
<h2>Posterior Inference for Regression 1</h2>
<ul>
<li><strong><em>observation</em> model</strong> relates the function we want to learn, <span class="math inline">\(f(x)\)</span>, to our observations <span class="math inline">\(y(x)\)</span>, both indexed by some input <span class="math inline">\(x\)</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>In classification, <span class="math inline">\(x\)</span> pixels of an image</li>
<li><span class="math inline">\(y\)</span> could be the associated class label.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In regression, <span class="math inline">\(y\)</span> typically represents a continuous output, such as a land surface temperature, a sea-level, a <span class="math inline">\(CO_2\)</span> concentration, etc.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In regression, we often assume the outputs are given by a latent noise-free function <span class="math inline">\(f(x)\)</span> plus i.i.d. Gaussian noise <span class="math inline">\(\epsilon(x)\)</span>:</li>
</ul>
<p><span class="math display">\[y(x) = f(x) + \epsilon(x),\]</span> :eqlabel:<code>eq_gp-regression</code></p>
<p>with <span class="math inline">\(\epsilon(x) \sim \mathcal{N}(0,\sigma^2)\)</span>. Let <span class="math inline">\(\mathbf{y} = y(X) = (y(x_1),\dots,y(x_n))^{\top}\)</span> be a vector of our training observations, and <span class="math inline">\(\textbf{f} = (f(x_1),\dots,f(x_n))^{\top}\)</span> be a vector of the latent noise-free function values, queried at the training inputs <span class="math inline">\(X = {x_1, \dots, x_n}\)</span>.</p>
</div>
</section>
<section id="posterior-inference-for-regression-2" class="slide level2">
<h2>Posterior Inference for Regression 2</h2>
<ul>
<li><p>assume <span class="math inline">\(f(x) \sim \mathcal{GP}(m,k)\)</span>, which means that any collection of function values <span class="math inline">\(\textbf{f}\)</span> has a joint multivariate Gaussian distribution, with mean vector <span class="math inline">\(\mu_i = m(x_i)\)</span> and covariance matrix <span class="math inline">\(K_{ij} = k(x_i,x_j)\)</span>. . . .</p></li>
<li><p><strong>RBF kernel</strong> <span class="math inline">\(k(x_i,x_j) = a^2 \exp\left(-\frac{1}{2\ell^2}||x_i-x_j||^2\right)\)</span> standard choice of covariance function</p></li>
<li><p>simplicity: assume the mean function <span class="math inline">\(m(x)=0\)</span>; our derivations can easily be generalized later on.</p></li>
</ul>
<div class="fragment">
<ul>
<li>want to make predictions at a set of inputs <span class="math display">\[X_* = x_{*1},x_{*2},\dots,x_{*m}.\]</span> Then we want to find <span class="math inline">\(x^2\)</span> and <span class="math inline">\(p(\mathbf{f}_* | \mathbf{y}, X)\)</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In the regression setting, we can conveniently find this distribution by using Gaussian identities, after finding the joint distribution over <span class="math inline">\(\mathbf{f}_* = f(X_*)\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>If we evaluate equation :eqref:<code>eq_gp-regression</code> at the training inputs <span class="math inline">\(X\)</span>, we have <span class="math inline">\(\mathbf{y} = \mathbf{f} + \mathbf{\epsilon}\)</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>By the definition of a GP (see last section), <span class="math inline">\(\mathbf{f} \sim \mathcal{N}(0,K(X,X))\)</span> where <span class="math inline">\(K(X,X)\)</span> is an <span class="math inline">\(n \times n\)</span> matrix formed by evaluating our covariance function (aka <em>kernel</em>) at all possible pairs of inputs <span class="math inline">\(x_i, x_j \in X\)</span></li>
</ul>
</div>
</section>
<section id="posterior-inference-for-regression-3" class="slide level2">
<h2>Posterior Inference for Regression 3</h2>
<ul>
<li><span class="math inline">\(\mathbf{\epsilon}\)</span> is simply a vector comprised of iid samples from <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span> and thus has distribution <span class="math inline">\(\mathcal{N}(0,\sigma^2I)\)</span>.</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is therefore a sum of two independent multivariate Gaussian variables, and thus has distribution <span class="math inline">\(\mathcal{N}(0, K(X,X) + \sigma^2I)\)</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>One can also show that <span class="math inline">\(\mathrm{cov}(\mathbf{f}_*, \mathbf{y}) = \mathrm{cov}(\mathbf{y},\mathbf{f}_*)^{\top} = K(X_*,X)\)</span> where <span class="math inline">\(K(X_*,X)\)</span> is an <span class="math inline">\(m \times n\)</span> matrix formed by evaluating the kernel at all pairs of test and training inputs.</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
\mathbf{y} \\
\mathbf{f}_*
\end{bmatrix}
\sim
\mathcal{N}\left(0,
\mathbf{A} = \begin{bmatrix}
K(X,X)+\sigma^2I &amp; K(X,X_*) \\
K(X_*,X) &amp; K(X_*,X_*)
\end{bmatrix}
\right)
\]</span></p>
</div>
<div class="fragment">
<ul>
<li>use standard Gaussian identities to find the conditional distribution from the joint distribution (see, e.g., Bishop Chapter 2), <span class="math inline">\(\mathbf{f}_* | \mathbf{y}, X, X_* \sim \mathcal{N}(m_*,S_*)\)</span>, where <span class="math inline">\(m_* = K(X_*,X)[K(X,X)+\sigma^2I]^{-1}\textbf{y}\)</span>, and <span class="math inline">\(S = K(X_*,X_*) - K(X_*,X)[K(X,X)+\sigma^2I]^{-1}K(X,X_*)\)</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>do not need to make use of the full predictive covariance matrix <span class="math inline">\(S\)</span>, and instead use the diagonal of <span class="math inline">\(S\)</span> for uncertainty about each prediction.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Often for this reason we write the predictive distribution for a single test point <span class="math inline">\(x_*\)</span>, rather than a collection of test points.</li>
</ul>
</div>
</section>
<section id="posterior-inference-for-regression-4" class="slide level2">
<h2>Posterior Inference for Regression 4</h2>
<ul>
<li><p>kernel matrix has parameters <span class="math inline">\(\theta\)</span> that we also wish to estimate, such the amplitude <span class="math inline">\(a\)</span> and lengthscale <span class="math inline">\(\ell\)</span> of the RBF kernel above.</p></li>
<li><p>For these purposes we use the <em>marginal likelihood</em>, <span class="math inline">\(p(\textbf{y} | \theta, X)\)</span>, which we already derived in working out the marginal distributions to find the joint distribution over <span class="math inline">\(\textbf{y},\textbf{f}_*\)</span>.</p></li>
<li><p>the <strong>marginal likelihood compartmentalizes into model fit and model complexity terms</strong>, and automatically encodes a notion of Occam’s razor for learning hyperparameters.</p></li>
<li><p>For a full discussion, see MacKay Ch. 28 :cite:<code>mackay2003information</code>, and Rasmussen and Williams Ch. 5 :cite:<code>rasmussen2006gaussian</code>.</p></li>
</ul>
</section>
<section id="equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression" class="slide level2">
<h2>Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression</h2>
<ul>
<li>equations you will use for learning hyperparameters and making predictions in Gaussian process regression:</li>
<li>assume a vector of regression targets <span class="math inline">\(\textbf{y}\)</span>, indexed by inputs <span class="math inline">\(X = \{x_1,\dots,x_n\}\)</span>, and we wish to make a prediction at a test input <span class="math inline">\(x_*\)</span>.</li>
<li>assume i.i.d. additive zero-mean Gaussian noise with variance <span class="math inline">\(\sigma^2\)</span>.</li>
<li>use a GP prior <span class="math inline">\(f(x) \sim \mathcal{GP}(m,k)\)</span> for the latent noise-free function, with mean function <span class="math inline">\(m\)</span> and kernel function <span class="math inline">\(k\)</span>.</li>
<li>kernel itself has parameters <span class="math inline">\(\theta\)</span> that we want to learn. For example, if we use an RBF kernel, <span class="math inline">\(k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)\)</span>, we want to learn <span class="math inline">\(\theta = \{a^2, \ell^2\}\)</span>.</li>
<li>Let <span class="math inline">\(K(X,X)\)</span> represent an <span class="math inline">\(n \times n\)</span> matrix corresponding to evaluating the kernel for all possible pairs of <span class="math inline">\(n\)</span> training inputs. Let <span class="math inline">\(K(x_*,X)\)</span> represent a <span class="math inline">\(1 \times n\)</span> vector formed by evaluating <span class="math inline">\(k(x_*, x_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li>Let <span class="math inline">\(\mu\)</span> be a mean vector formed by evaluating the mean function <span class="math inline">\(m(x)\)</span> at every training points <span class="math inline">\(x\)</span>.</li>
</ul>
</section>
<section id="equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression-2" class="slide level2">
<h2>Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression 2</h2>
<ul>
<li>Typically in working with Gaussian processes, we follow a two-step procedure.</li>
</ul>
<ol type="1">
<li>Learn kernel hyperparameters <span class="math inline">\(\hat{\theta}\)</span> by maximizing the marginal likelihood with respect to these hyperparameters.</li>
<li>Use the predictive mean as a point predictor, and 2 times the predictive standard deviation to form a 95% credible set, conditioning on these learned hyperparameters <span class="math inline">\(\hat{\theta}\)</span>.</li>
</ol>
<p>The log marginal likelihood is simply a log Gaussian density, which has the form: <span class="math display">\[\log p(\textbf{y} | \theta, X) = -\frac{1}{2}\textbf{y}^{\top}[K_{\theta}(X,X) + \sigma^2I]^{-1}\textbf{y} - \frac{1}{2}\log|K_{\theta}(X,X)| + c\]</span></p>
</section>
<section id="equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression-3" class="slide level2">
<h2>Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression 3</h2>
<p>The predictive distribution has the form: <span class="math display">\[p(y_* | x_*, \textbf{y}, \theta) = \mathcal{N}(a_*,v_*)\]</span> <span class="math display">\[a_* = k_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}(\textbf{y}-\mu) + \mu\]</span> <span class="math display">\[v_* = k_{\theta}(x_*,x_*) - K_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}k_{\theta}(X,x_*)\]</span></p>
</section>
<section id="interpreting-equations-for-learning-and-predictions-1" class="slide level2">
<h2>Interpreting Equations for Learning and Predictions 1</h2>
<ul>
<li><p><strong>key points to note</strong> about the predictive distributions for Gaussian processes:</p></li>
<li><p>Despite the flexibility of the model class: <strong>possible to do <em>exact</em> Bayesian inference for GP regression in <em>closed form</em></strong>.</p></li>
<li><p>Aside from learning the kernel hyperparameters, there is no <em>training</em>.</p></li>
<li><p>can <strong>write down exactly what equations we want to use to make predictions</strong>.</p></li>
<li><p>GPs <strong>relatively exceptional in this respect</strong>, and it has greatly contributed to their <strong>convenience, versatility, and continued popularity</strong>.</p></li>
<li><p>The predictive mean <span class="math inline">\(a_*\)</span> is a linear combination of the training targets <span class="math inline">\(\textbf{y}\)</span>, weighted by the kernel <span class="math inline">\(k_{\theta}(x_*,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}\)</span>.</p></li>
<li><p><strong>kernel (and its hyperparameters) thus plays a crucial role in the generalization properties of the model</strong>.</p></li>
<li><p>The predictive mean explicitly depends on the target values <span class="math inline">\(\textbf{y}\)</span> but the predictive variance does not. The predictive uncertainty instead grows as the test input <span class="math inline">\(x_*\)</span> moves away from the target locations <span class="math inline">\(X\)</span>, as governed by the kernel function.</p></li>
<li><p>However, <strong>uncertainty will implicitly depend on the values of the targets <span class="math inline">\(\textbf{y}\)</span> through the kernel hyperparameters <span class="math inline">\(\theta\)</span>, which are learned from the data</strong>.</p></li>
</ul>
</section>
<section id="interpreting-equations-for-learning-and-predictions-2" class="slide level2">
<h2>Interpreting Equations for Learning and Predictions 2</h2>
<ul>
<li><p>marginal likelihood compartmentalizes into model fit and model complexity (log determinant) terms.</p></li>
<li><p>marginal likelihood tends to select for hyperparameters that provide the simplest fits that are still consistent with the data.</p></li>
<li><p><strong>key computational bottlenecks come from solving a linear system and computing a log determinant over an <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix <span class="math inline">\(K(X,X)\)</span> for <span class="math inline">\(n\)</span> training points.</strong></p></li>
<li><p>Naively, <span class="math inline">\(\mathcal{O}(n^3)\)</span> computations and <span class="math inline">\(\mathcal{O}(n^2)\)</span> storage for each entry of the kernel (covariance) matrix</p></li>
<li><p>Historically, these bottlenecks have <strong>limited GPs to problems with fewer than about 10,000 training points</strong></p></li>
<li><p><strong>reputation for “being slow”</strong> that has been inaccurate now for almost a decade.</p></li>
<li><p><strong>GPs can be scaled to problems with millions of points</strong>.</p></li>
<li><p>popular choices of kernel functions, <span class="math inline">\(K(X,X)\)</span> is often close to singular, which can cause numerical issues when performing Cholesky decompositions or other operations intended to solve linear systems.</p></li>
<li><p>Fortunately, in regression we are often working with <span class="math inline">\(K_{\theta}(X,X)+\sigma^2I\)</span>, such that the noise variance <span class="math inline">\(\sigma^2\)</span> gets added to the diagonal of <span class="math inline">\(K(X,X)\)</span>, significantly improving its conditioning.</p></li>
<li><p>If the noise variance is small, or we are doing noise free regression, it is common practice to add a small amount of “jitter” to the diagonal, on the order of <span class="math inline">\(10^{-6}\)</span>, to improve conditioning.</p></li>
</ul>
</section>
<section id="worked-example-from-scratch-1" class="slide level2">
<h2>Worked Example from Scratch 1</h2>
<p>Let’s create some regression data, and then fit the data with a GP, implementing every step from scratch. We’ll sample data from <span class="math display">\[y(x) = \sin(x) + \frac{1}{2}\sin(4x) + \epsilon,\]</span> with <span class="math inline">\(\epsilon \sim \mathcal{N}(0,\sigma^2)\)</span>. The noise free function we wish to find is <span class="math inline">\(f(x) = \sin(x) + \frac{1}{2}\sin(4x)\)</span>. We’ll start by using a noise standard deviation <span class="math inline">\(\sigma = 0.25\)</span>.</p>

<img data-src="./img/output_gp-inference_714770_3_0.svg" style="width:100.0%" class="r-stretch"></section>
<section id="worked-example-from-scratch-2" class="slide level2">
<h2>Worked Example from Scratch 2</h2>
<p>Here we see the noisy observations as circles, and the noise-free function in blue that we wish to find.</p>
<p>Now, let’s specify a GP prior over the latent noise-free function, <span class="math inline">\(f(x)\sim \mathcal{GP}(m,k)\)</span>. We’ll use a mean function <span class="math inline">\(m(x) = 0\)</span>, and an RBF covariance function (kernel) <span class="math display">\[k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right).\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>mean <span class="op">=</span> np.zeros(test_x.shape[<span class="dv">0</span>])</span>
<span id="cb1-2"><a href="#cb1-2"></a>cov <span class="op">=</span> d2l.rbfkernel(test_x, test_x, ls<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We have started with a length-scale of 0.2. Before we fit the data, it is important to consider whether we have specified a reasonable prior. Let’s visualize some sample functions from this prior, as well as the 95% credible set (we believe there’s a 95% chance that the true function is within this region).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>prior_samples <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>cov, size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a>d2l.plt.plot(test_x, prior_samples.T, color<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a>d2l.plt.plot(test_x, mean, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>d2l.plt.fill_between(test_x, mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.diag(cov), mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.diag(cov), </span>
<span id="cb2-5"><a href="#cb2-5"></a>                 alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="worked-example-from-scratch-3" class="slide level2">
<h2>Worked Example from Scratch 3</h2>
<p>Do these samples look reasonable? Are the high-level properties of the functions aligned with the type of data we are trying to model?</p>
<p>Now let’s form the mean and variance of the posterior predictive distribution at any arbitrary test point <span class="math inline">\(x_*\)</span>.</p>
<p><span class="math display">\[
\bar{f}_{*} = K(x, x_*)^T (K(x, x) + \sigma^2 I)^{-1}y
\]</span></p>
<p><span class="math display">\[
V(f_{*}) = K(x_*, x_*) - K(x, x_*)^T (K(x, x) + \sigma^2 I)^{-1}K(x, x_*)
\]</span></p>
<p>Before we make predictions, we should learn our kernel hyperparameters <span class="math inline">\(\theta\)</span> and noise variance <span class="math inline">\(\sigma^2\)</span>. Let’s initialize our length-scale at 0.75, as our prior functions looked too quickly varying compared to the data we are fitting. We’ll also guess a noise standard deviation <span class="math inline">\(\sigma\)</span> of 0.75.</p>
<p>In order to learn these parameters, we will maximize the marginal likelihood with respect to these parameters.</p>
<p><span class="math display">\[
\log p(y | X) = \log \int p(y | f, X)p(f | X)df
\]</span> <span class="math display">\[
\log p(y | X) = -\frac{1}{2}y^T(K(x, x) + \sigma^2 I)^{-1}y - \frac{1}{2}\log |K(x, x) + \sigma^2 I| - \frac{n}{2}\log 2\pi
\]</span></p>
</section>
<section id="worked-example-from-scratch-4" class="slide level2">
<h2>Worked Example from Scratch 4</h2>
<p>Perhaps our prior functions were too quickly varying. Let’s guess a length-scale of 0.4. We’ll also guess a noise standard deviation of 0.75. These are simply hyperparameter initializations — we will learn these parameters from the marginal likelihood.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>ell_est <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>post_sig_est <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="kw">def</span> neg_MLL(pars):</span>
<span id="cb3-5"><a href="#cb3-5"></a>    K <span class="op">=</span> d2l.rbfkernel(train_x, train_x, ls<span class="op">=</span>pars[<span class="dv">0</span>])</span>
<span id="cb3-6"><a href="#cb3-6"></a>    kernel_term <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> train_y <span class="op">@</span> <span class="op">\</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>        np.linalg.inv(K <span class="op">+</span> pars[<span class="dv">1</span>] <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>])) <span class="op">@</span> train_y</span>
<span id="cb3-8"><a href="#cb3-8"></a>    logdet <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.log(np.linalg.det(K <span class="op">+</span> pars[<span class="dv">1</span>] <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>                                         np.eye(train_x.shape[<span class="dv">0</span>])))</span>
<span id="cb3-10"><a href="#cb3-10"></a>    const <span class="op">=</span> <span class="op">-</span>train_x.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="fl">2.</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb3-11"><a href="#cb3-11"></a>    </span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="cf">return</span> <span class="op">-</span>(kernel_term <span class="op">+</span> logdet <span class="op">+</span> const)</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a>learned_hypers <span class="op">=</span> optimize.minimize(neg_MLL, x0<span class="op">=</span>np.array([ell_est,post_sig_est]), </span>
<span id="cb3-16"><a href="#cb3-16"></a>                                   bounds<span class="op">=</span>((<span class="fl">0.01</span>, <span class="fl">10.</span>), (<span class="fl">0.01</span>, <span class="fl">10.</span>)))</span>
<span id="cb3-17"><a href="#cb3-17"></a>ell <span class="op">=</span> learned_hypers.x[<span class="dv">0</span>]</span>
<span id="cb3-18"><a href="#cb3-18"></a>post_sig_est <span class="op">=</span> learned_hypers.x[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this instance, we learn a length-scale of 0.299, and a noise standard deviation of 0.24. Note that the learned noise is extremely close to the true noise, which helps indicate that our GP is a very well-specified to this problem.</p>
</section>
<section id="worked-example-from-scratch-5" class="slide level2">
<h2>Worked Example from Scratch 5</h2>
<ul>
<li><strong>crucial to put careful thought into selecting the kernel and initializing the hyperparameters</strong></li>
<li><strong>not immune to poor initializations</strong></li>
</ul>
<p>Now, let’s make predictions with these learned hypers.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>K_x_xstar <span class="op">=</span> d2l.rbfkernel(train_x, test_x, ls<span class="op">=</span>ell)</span>
<span id="cb4-2"><a href="#cb4-2"></a>K_x_x <span class="op">=</span> d2l.rbfkernel(train_x, train_x, ls<span class="op">=</span>ell)</span>
<span id="cb4-3"><a href="#cb4-3"></a>K_xstar_xstar <span class="op">=</span> d2l.rbfkernel(test_x, test_x, ls<span class="op">=</span>ell)</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>post_mean <span class="op">=</span> K_x_xstar.T <span class="op">@</span> np.linalg.inv((K_x_x <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>                post_sig_est <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>]))) <span class="op">@</span> train_y</span>
<span id="cb4-7"><a href="#cb4-7"></a>post_cov <span class="op">=</span> K_xstar_xstar <span class="op">-</span> K_x_xstar.T <span class="op">@</span> np.linalg.inv((K_x_x <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>                post_sig_est <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> np.eye(train_x.shape[<span class="dv">0</span>]))) <span class="op">@</span> K_x_xstar</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a>lw_bd <span class="op">=</span> post_mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov))</span>
<span id="cb4-11"><a href="#cb4-11"></a>up_bd <span class="op">=</span> post_mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov))</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>d2l.plt.scatter(train_x, train_y)</span>
<span id="cb4-14"><a href="#cb4-14"></a>d2l.plt.plot(test_x, test_y, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb4-15"><a href="#cb4-15"></a>d2l.plt.plot(test_x, post_mean, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb4-16"><a href="#cb4-16"></a>d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb4-17"><a href="#cb4-17"></a>d2l.plt.legend([<span class="st">'Observed Data'</span>, <span class="st">'True Function'</span>, <span class="st">'Predictive Mean'</span>, <span class="st">'95% Set on True Func'</span>])</span>
<span id="cb4-18"><a href="#cb4-18"></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We see the posterior mean in orange almost perfectly matches the true noise free function! Note that the 95% credible set we are showing is for the latent <em>noise free</em> (true) function, and not the data points. We see that this credible set entirely contains the true function, and does not seem overly wide or narrow. We would not want nor expect it to contain the data points.</p>
</section>
<section id="worked-example-from-scratch-6" class="slide level2">
<h2>Worked Example from Scratch 6</h2>
<p>If we wish to have a credible set for the observations, we should compute</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>lw_bd_observed <span class="op">=</span> post_mean <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov) <span class="op">+</span> post_sig_est <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2"></a>up_bd_observed <span class="op">=</span> post_mean <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sqrt(np.diag(post_cov) <span class="op">+</span> post_sig_est <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>two sources of uncertainty</p></li>
<li><p><em>epistemic</em> uncertainty, representing <em>reducible</em> uncertainty</p></li>
<li><p><em>aleatoric</em> or <em>irreducible</em> uncertainty</p></li>
<li><p><em>epistemic</em> uncertainty here represents uncertainty about the true values of the noise free function. This uncertainty should grow as we move away from the data points, as away from the data there are a greater variety of function values consistent with our data. As we observe more and more data, our beliefs about the true function become more confident, and the epistemic uncertainty disappears</p></li>
<li><p><em>aleatoric</em> uncertainty in this instance is the observation noise, since the data are given to us with this noise, and it cannot be reduced.</p></li>
<li><p><em>epistemic</em> uncertainty in the data is captured by variance of the latent noise free function np.diag(post_cov)</p></li>
<li><p><em>aleatoric</em> uncertainty is captured by the noise variance post_sig_est**2.</p></li>
</ul>
</section>
<section id="worked-example-from-scratch-7" class="slide level2">
<h2>Worked Example from Scratch 7</h2>
<ul>
<li>people often careless about how they represent uncertainty, with many papers showing error bars that are completely undefined, no clear sense of whether we are visualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with noise standard deviations, standard deviations with standard errors, confidence intervals with credible sets, and so on</li>
<li>Without being precise about what the uncertainty represents, it is essentially meaningless.</li>
<li>crucial to note that we are taking <em>two times</em> the <em>square root</em> of our variance estimate for the noise free function.</li>
<li>predictive distribution is Gaussian –&gt; enables us to form a 95% credible set, representing our beliefs about the interval which is 95% likely to contain the ground truth function. The noise <em>variance</em> is living on a completely different scale, and is much less interpretable.</li>
</ul>
<p>Finally, let’s take a look at 20 posterior samples. These samples tell us what types of functions we believe might fit our data, a posteriori.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>post_samples <span class="op">=</span> np.random.multivariate_normal(post_mean, post_cov, size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a>d2l.plt.scatter(train_x, train_y)</span>
<span id="cb6-3"><a href="#cb6-3"></a>d2l.plt.plot(test_x, test_y, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb6-4"><a href="#cb6-4"></a>d2l.plt.plot(test_x, post_mean, linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>d2l.plt.plot(test_x, post_samples.T, color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>plt.legend([<span class="st">'Observed Data'</span>, <span class="st">'True Function'</span>, <span class="st">'Predictive Mean'</span>, <span class="st">'Posterior Samples'</span>])</span>
<span id="cb6-8"><a href="#cb6-8"></a>d2l.plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="worked-example-from-scratch-8" class="slide level2">
<h2>Worked Example from Scratch 8</h2>
<ul>
<li><strong>basic regression applications: most common to use the posterior predictive mean and standard deviation as a point predictor</strong> and metric for uncertainty, respectively</li>
<li><strong>more advanced applications</strong> such as Bayesian optimization with Monte Carlo acquisition functions, or Gaussian processes for model-based RL: <strong>often necessary to take posterior samples</strong>.</li>
<li>However, even if not strictly required in the basic applications, these samples give us more intuition about the fit we have for the data, and are often useful to include in visualizations.</li>
</ul>
</section>
<section id="making-life-easy-with-gpytorch-1" class="slide level2">
<h2>Making Life Easy with GPyTorch 1</h2>
<ul>
<li><p>easy to implement basic GP regression entirely from scratch.</p></li>
<li><p>if we want to <strong>explore a variety of kernel choices</strong>, consider approximate inference (which is needed even for classification), combine GPs with neural networks, or even have a dataset larger than about 10,000 points, then an <strong>implementation from scratch becomes unwieldy and cumbersome</strong>.</p></li>
<li><p>Some of the most effective methods for scalable GP inference, such as SKI (also known as KISS-GP), can require hundreds of lines of code implementing advanced numerical linear algebra routines.</p></li>
<li><p>–&gt; use <em>GPyTorch</em> library</p></li>
</ul>
</section>
<section id="making-life-easy-with-gpytorch-2" class="slide level2">
<h2>Making Life Easy with GPyTorch 2</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># First let's convert our data into tensors for use with PyTorch</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>train_x <span class="op">=</span> torch.tensor(train_x)</span>
<span id="cb7-3"><a href="#cb7-3"></a>train_y <span class="op">=</span> torch.tensor(train_y)</span>
<span id="cb7-4"><a href="#cb7-4"></a>test_y <span class="op">=</span> torch.tensor(test_y)</span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co"># We are using exact GP inference with a zero mean and RBF kernel</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="kw">class</span> ExactGPModel(gpytorch.models.ExactGP):</span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train_x, train_y, likelihood):</span>
<span id="cb7-9"><a href="#cb7-9"></a>        <span class="bu">super</span>(ExactGPModel, <span class="va">self</span>).<span class="fu">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb7-10"><a href="#cb7-10"></a>        <span class="va">self</span>.mean_module <span class="op">=</span> gpytorch.means.ZeroMean()</span>
<span id="cb7-11"><a href="#cb7-11"></a>        <span class="va">self</span>.covar_module <span class="op">=</span> gpytorch.kernels.ScaleKernel(</span>
<span id="cb7-12"><a href="#cb7-12"></a>            gpytorch.kernels.RBFKernel())</span>
<span id="cb7-13"><a href="#cb7-13"></a>    </span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-15"><a href="#cb7-15"></a>        mean_x <span class="op">=</span> <span class="va">self</span>.mean_module(x)</span>
<span id="cb7-16"><a href="#cb7-16"></a>        covar_x <span class="op">=</span> <span class="va">self</span>.covar_module(x)</span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="cf">return</span> gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>puts the data in the right format for GPyTorch</li>
<li>specifies that we are using exact inference, as well the mean function (zero) and kernel function (RBF) that we want to use</li>
<li>can use any other kernel very easily, by calling, for instance, gpytorch.kernels.matern_kernel(), or gpyotrch.kernels.spectral_mixture_kernel()</li>
<li>So far, we have only discussed exact inference, where it is possible to infer a predictive distribution without making any approximations.</li>
</ul>
</section>
<section id="making-life-easy-with-gpytorch-3" class="slide level2">
<h2>Making Life Easy with GPyTorch 3</h2>
<ul>
<li>for GPs, we <strong>can only perform exact inference when we have a Gaussian likelihood</strong></li>
<li>when we assume that our observations are generated as a noise-free function represented by a Gaussian process, plus Gaussian noise.</li>
<li><strong>future: consider other settings, such as classification, where we cannot make these assumptions</strong>.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Initialize Gaussian likelihood</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb8-3"><a href="#cb8-3"></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb8-4"><a href="#cb8-4"></a>training_iter <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co"># Find optimal model hyperparameters</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>model.train()</span>
<span id="cb8-7"><a href="#cb8-7"></a>likelihood.train()</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Use the adam optimizer, includes GaussianLikelihood parameters</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)  </span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="co"># Set our loss as the negative log GP marginal likelihood</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>explicitly specify the likelihood we want to use (Gaussian)</li>
<li>objective we will use for training kernel hyperparameters (here, the marginal likelihood)</li>
<li>the procedure we we want to use for optimizing that objective (in this case, Adam)</li>
<li>note that while we are using Adam, which is a “stochastic” optimizer, in this case, it is <strong>full-batch Adam</strong>.</li>
</ul>
</section>
<section id="making-life-easy-with-gpytorch-4" class="slide level2">
<h2>Making Life Easy with GPyTorch 4</h2>
<ul>
<li><p>marginal likelihood does not factorize over data instances, we <strong>cannot use an optimizer over “mini-batches” of data</strong> and be guaranteed convergence. Other optimizers, such as L-BFGS, are also supported by GPyTorch.</p></li>
<li><p><strong>doing a good job of optimizing the marginal likelihood corresponds strongly with good generalization</strong> –&gt; use powerful optimizers like L-BFGS (2nd order)</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_iter):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="co"># Zero gradients from previous iteration</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>    optimizer.zero_grad()</span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="co"># Output from model</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb9-6"><a href="#cb9-6"></a>    <span class="co"># Calc loss and backprop gradients</span></span>
<span id="cb9-7"><a href="#cb9-7"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb9-8"><a href="#cb9-8"></a>    loss.backward()</span>
<span id="cb9-9"><a href="#cb9-9"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-10"><a href="#cb9-10"></a>        <span class="bu">print</span>(<span class="ss">f'Iter </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">:d}</span><span class="ss">/</span><span class="sc">{</span>training_iter<span class="sc">:d}</span><span class="ss"> - Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss"> '</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>              <span class="ss">f'squared lengthscale: '</span></span>
<span id="cb9-12"><a href="#cb9-12"></a>              <span class="ss">f'</span><span class="sc">{</span>model<span class="sc">.</span>covar_module<span class="sc">.</span>base_kernel<span class="sc">.</span>lengthscale<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss"> '</span></span>
<span id="cb9-13"><a href="#cb9-13"></a>              <span class="ss">f'noise variance: </span><span class="sc">{</span>model<span class="sc">.</span>likelihood<span class="sc">.</span>noise<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb9-14"><a href="#cb9-14"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here we actually run the optimization procedure, outputting the values of the loss every 10 iterations.</p>
</section>
<section id="making-life-easy-with-gpytorch-5" class="slide level2">
<h2>Making Life Easy with GPyTorch 5</h2>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Get into evaluation (predictive posterior) mode</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>test_x <span class="op">=</span> torch.tensor(test_x)</span>
<span id="cb10-3"><a href="#cb10-3"></a>model.<span class="bu">eval</span>()</span>
<span id="cb10-4"><a href="#cb10-4"></a>likelihood.<span class="bu">eval</span>()</span>
<span id="cb10-5"><a href="#cb10-5"></a>observed_pred <span class="op">=</span> likelihood(model(test_x)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above codeblock enables us to make predictions on our test inputs.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python input number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="co"># Initialize plot</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>    f, ax <span class="op">=</span> d2l.plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb11-4"><a href="#cb11-4"></a>    <span class="co"># Get upper and lower bounds for 95\% credible set (in this case, in</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="co"># observation space)</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>    lower, upper <span class="op">=</span> observed_pred.confidence_region()</span>
<span id="cb11-7"><a href="#cb11-7"></a>    ax.scatter(train_x.numpy(), train_y.numpy())</span>
<span id="cb11-8"><a href="#cb11-8"></a>    ax.plot(test_x.numpy(), test_y.numpy(), linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), linewidth<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb11-10"><a href="#cb11-10"></a>    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb11-11"><a href="#cb11-11"></a>    ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb11-12"><a href="#cb11-12"></a>    ax.legend([<span class="st">'True Function'</span>, <span class="st">'Predictive Mean'</span>, <span class="st">'Observed Data'</span>,</span>
<span id="cb11-13"><a href="#cb11-13"></a>               <span class="st">'95% Credible Set'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="making-life-easy-with-gpytorch-5-1" class="slide level2">
<h2>Making Life Easy with GPyTorch 5</h2>
<p>Finally, we plot the fit.</p>

<img data-src="./img/output_gp-inference_714770_25_0.svg" style="width:100.0%" class="r-stretch"><ul>
<li>fits are virtually identical</li>
<li>few things to note: GPyTorch is working with <em>squared</em> length-scales and observation noise</li>
<li>For example, our learned noise standard deviation in the for scratch code is about 0.283</li>
<li>The noise variance found by GPyTorch is <span class="math inline">\(0.81 \approx 0.283^2\)</span></li>
<li>In the GPyTorch plot, we also show the credible set in the <em>observation space</em> rather than the latent function space, to demonstrate that they indeed cover the observed datapoints.</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>can <strong>combine a Gaussian process prior with data to form a posterior</strong>, which we use to make predictions</li>
<li>can <strong>also form a marginal likelihood</strong>, which is <strong>useful for automatic learning of kernel hyperparameters</strong>, which control properties such as the rate of variation of the Gaussian process</li>
<li><strong>mechanics of forming the posterior and learning kernel hyperparameters for regression are simple</strong>, involving about a dozen lines of code</li>
<li><strong>introduced the GPyTorch library</strong>.</li>
<li>Although GPyTorch code relatively long, can be trivially modified for other kernel functions, or more advanced functionality</li>
</ul>
</section>
<section id="exercises-1" class="slide level2">
<h2>Exercises 1</h2>
<ol type="1">
<li>We have emphasized the importance of <em>learning</em> kernel hyperparameters, and the effect of hyperparameters and kernels on the generalization properties of Gaussian processes. Try skipping the step where we learn hypers, and instead guess a variety of length-scales and noise variances, and check their effect on predictions. What happens when you use a large length-scale? A small length-scale? A large noise variance? A small noise variance?</li>
<li>We have said that the marginal likelihood is not a convex objective, but that hyperparameters like length-scale and noise variance can be reliably estimated in GP regression. This is generally true — in fact, the marginal likelihood is <em>much</em> better at learning length-scale hyperparameters than conventional approaches in spatial statistics, which involve fitting empirical autocorrelation functions (“covariograms”). Arguably, the biggest contribution from machine learning to Gaussian process research, at least before recent work on scalable inference, was the introduction of the marginal lkelihood for hyperparameter learning.</li>
</ol>
<p><em>However</em>, different pairings of even these parameters provide interpretably different plausible explanations for many datasets, leading to local optima in our objective. If we use a large length-scale, then we assume the true underlying function is slowly varying. If the observed data <em>are</em> varying significantly, then the only we can plausibly have a large length-scale is with a large noise-variance. If we use a small length-scale, on the other hand, our fit will be very sensitive to the variations in the data, leaving little room to explain variations with noise (aleatoric uncertainty).</p>
</section>
<section id="exercises-2" class="slide level2">
<h2>Exercises 2</h2>
<p>Try seeing if you can find these local optima: initialize with very large length-scale with large noise, and small length-scales with small noise. Do you converge to different solutions?</p>
<ol start="3" type="1">
<li><p>We have said that a fundamental advantage of Bayesian methods is in naturally representing <em>epistemic</em> uncertainty. In the above example, we cannot fully see the effects of epistemic uncertainty. Try instead to predict with <code>test_x = np.linspace(0, 10, 1000)</code>. What happens to the 95% credible set as your predictions move beyond the data? Does it cover the true function in that interval? What happens if you only visualize aleatoric uncertainty in that region?</p></li>
<li><p>Try running the above example, but instead with 10,000, 20,000 and 40,000 training points, and measure the runtimes. How does the training time scale? Alternatively, how do the runtimes scale with the number of test points? Is it different for the predictive mean and the predictive variance? Answer this question both by theoretically working out the training and testing time complexities, and by running the code above with a different number of points.</p></li>
<li><p>Try running the GPyTorch example with different covariance functions, such as the Matern kernel. How do the results change? How about the spectral mixture kernel, found in the GPyTorch library? Are some easier to train the marginal likelihood than others? Are some more valuable for long-range versus short-range predictions?</p></li>
</ol>
</section>
<section id="exercises-3" class="slide level2">
<h2>Exercises 3</h2>
<ol start="6" type="1">
<li>In our GPyTorch example, we plotted the predictive distribution including observation noise, while in our “from scratch” example, we only included epistemic uncertainty. Re-do the GPyTorch example, but this time only plotting epistemic uncertainty, and compare to the from-scratch results. Do the predictive distributions now look the same? (They should.)</li>
</ol>
<p>:begin_tab:<code>pytorch</code> <a href="https://discuss.d2l.ai/t/12117">Discussions</a> :end_tab:</p>

<img src="eclipse_logo_small.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://github.com/ECLIPSE-Lab/SS23_DataScienceForEM">SS23_DataScienceForEM</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="03_gp_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="03_gp_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="03_gp_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="03_gp_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>