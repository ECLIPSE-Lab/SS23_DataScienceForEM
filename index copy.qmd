---
format: 
  revealjs:
    theme: ["theme/q-theme.scss"]
    slide-number: c/t
    logo: "https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png"
    footer: "[https://jthomasmock.github.io/arrow-dplyr](https://jthomasmock.github.io/arrow-dplyr)"
    code-copy: true
    center-title-slide: false
    include-in-header: heading-meta.html
    code-link: true
    code-overflow: wrap
    highlight-style: a11y
    height: 1080
    width: 1920
execute: 
  eval: true
  echo: true
---

<h1> Outrageously efficient<br>exploratory data analysis </h1>

<h2> with Apache Arrow and `dplyr` </h2>

<hr>

<h3> Tom Mock, Customer Enablement Lead at </h3>

<h3> 2022-06-03 </h3>
<br>

<h3> `r fontawesome::fa("github", "black")` &nbsp; [github.com/jthomasmock/arrow-dplyr](https://github.com/jthomasmock/arrow-dplyr)

![](https://arrow.apache.org/img/offbrand_hex_2.png){.absolute top=425 left=1100 width="300"}
![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/dplyr.png){.absolute top=680 left=1250 width="300"}

![](https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png){.absolute top=360 left=965 height="65"}

## `arrow`

> `arrow` is software development platform for building high performance applications that process and transport large data sets  

. . .

* The `arrow` R package is an interface to data via the `arrow` backend, and has deep integration with `dplyr`:  
  - Ungrouped `mutate()`, `filter()`, `select()` was available in `arrow` 5.0  
  - `group_by()` + `summarize()` aggregation was added in `arrow` 6.0  
  - More complex `dplyr` operations were added in `arrow`7.0 and 8.0
  
. . .

* `arrow` data can also be "handed off" to `duckdb` with `to_duckdb()` for any `dbplyr` commands without data conversion. IE no serialization or data copying costs are incurred. 

## Working with bigger data?

* Relational databases (IE SQL) are still around and hugely popular but...  

. . .

* Data and specifically _local_ files are getting bigger  

. . .

* Additionally, many Data Warehouses/Data Lakes use flat-file storage (`.csv`, `.parquet`, `.json` etc) - there are query engines in many environments, but you can often end up with large extracts.

. . .

So, how do you work with data extracts that aren't already in a database, and are bigger than your memory?

## Pause for one second

If it _can_ fit in memory, then try out:  

* [`vroom::vroom()`](https://vroom.r-lib.org/) or [`data.table::fread()`](https://rdatatable.gitlab.io/data.table/reference/fread.html) for fast file reads _into_ R  
  * [`vroom(col_select = c(column_name))`](https://vroom.r-lib.org/reference/vroom.html) also allows for partial reads (ie specific columns)  
  * `arrow` itself also has crazy fast file reads on many file types

. . .

* [`data.table`](https://rdatatable.gitlab.io/data.table/index.html) or the `dplyr` front-end to `data.table` via [`dtplyr`](https://dtplyr.tidyverse.org/) for fast and efficient in-memory analysis  

. . .

* Lastly, the [`collapse`](https://sebkrantz.github.io/collapse/) R package for limited capability, but hyper-performant data manipulation  

## Fill your quiver with `arrow`s




[Using `tictoc` to [watch cute dog videos]{.fragment .strike fragment-index=2}]{.fragment fragment-index=1} [time our computations.]{.fragment fragment-index=2}


:::: {.columns .fragment fragment-index=1}

::: {.column width="50%"}

![](https://media0.giphy.com/media/EOIQA7mySTulzp7PoR/giphy.gif?cid=ecf05e47o0cq51yelvgpewdyud8ya013gejduf351z5233lq&rid=giphy.gif&ct=g){width="600"}
![](https://media4.giphy.com/media/DIrGyd84DwA48/giphy.gif?cid=ecf05e47977mxiw5gkfbd23znh0gxgra4kcqxa82c5e0laqs&rid=giphy.gif&ct=g){width="600"}

:::

::: {.column width="50%"}
![](https://media0.giphy.com/media/348lrWhJAUiuhkcePl/giphy.gif?cid=ecf05e476kk82d9kk7u3rub2wec0qpz2c32kxt2504hf1p7v&rid=giphy.gif&ct=g){width="600"}
![](https://media4.giphy.com/media/OVz2F93KQzSqQ/giphy.gif?cid=ecf05e476kk82d9kk7u3rub2wec0qpz2c32kxt2504hf1p7v&rid=giphy.gif&ct=g){width="600"}
:::

::::

## On to `arrow`

There are great examples of data analysis on big data (2 billion rows) in the [`arrow` docs](https://arrow.apache.org/docs/r/articles/dataset.html). 

. . .

For today, I'm going to focus on biggish data but manageable data!

. . .

<hr>

If we were to use CSVs, this would be about 2.19 GB of data



. . .

But because we're using `arrow`, we can use more efficient parquet files. This data on disk is about 82% smaller at 388 MB.



. . .

So not _THAT_ big but 372 columns and 1.1 million rows is plenty.







## `nflfastR` data

The data we're focused on today is big enough (1 million rows by 372 columns, about 2.2 GB uncompressed) and corresponds to _every_ NFL play in _every_ game from 1999 to 2021. The data is all available from the [{nflverse} Github](https://github.com/nflverse/nflfastR-data).

. . .

To use it efficiently, we've partitioned the data up into each season

:::: {.columns}

::: {.column width="50%"}

```
data-parquet/
├── 1999
│   └── data.parquet
├── 2000
│   └── data.parquet
├── 2001
│   └── data.parquet
├── 2002
│   └── data.parquet
├── 2003
│   └── data.parquet
├── 2004
│   └── data.parquet
├── 2005
│   └── data.parquet
├── 2006
│   └── data.parquet
├── 2007
│   └── data.parquet
├── 2008
│   └── data.parquet
├── 2009
│   └── data.parquet
├── 2010
│   └── data.parquet
├── 2011
│   └── data.parquet
├── 2012
│   └── data.parquet
├── 2013
│   └── data.parquet
├── 2014
│   └── data.parquet
```

:::

::: {.column width="50%"}

```

├── 2015
│   └── data.parquet
├── 2016
│   └── data.parquet
├── 2017
│   └── data.parquet
├── 2018
│   └── data.parquet
├── 2019
│   └── data.parquet
├── 2020
│   └── data.parquet
└── 2021
    └── data.parquet
```

:::

::::

## Nock the `arrow`

We can prepare our data to be used with `arrow::open_dataset()`



## Pull the `arrow` back with `dplyr`



. . .

Note that while the computation has occurred, we really haven't "seen it" yet. Printing just reports back the 3x columns and their type.

## Release the `arrow` into memory with `collect()`

We can execute the `collect()` function to _finally_ pull the output into memory and display the result.

. . .



## Release the `arrow` into memory with `collect()`

Once it's pulled into memory, it's like any other in-memory object!

. . .



## Bigger and faster

We can operate across all the rows extremely quickly!




## Better exploratory data analysis

While `arrow` + `dplyr` can be combined for extremely efficient and fast data analysis, having to `collect()` into memory when the results may be very large is not ideal.

. . .

Enter the [`arrow::to_duckdb()` function](https://arrow.apache.org/docs/r/reference/to_duckdb.html)! This is essentially a zero-cost operation that will treat the on-disk data in place as a [`duckdb` database](https://duckdb.org/)!

. . .







## More `arrow`s for more `duckdb`s

:::: {.columns}

::: {.column width="45%"}





`lazy query [?? x 4]` indicates a `dbplyr` connection, prints 10 rows and the remaining dataset hasn't been pulled into memory yet!

:::

::: {.column width="45%"}

:::

::::

## More `arrow`s for more `duckdb`s

:::: {.columns}

::: {.column width="45%"}




`lazy query [?? x 4]` indicates a `dbplyr` connection, prints 10 rows and the remaining dataset hasn't been pulled into memory yet!

:::


::: {.column width="45%"}



We can easily print more!

:::

::::



## Rapid fire question -> answer

Just like with `dplyr` in memory, you can write and answer queries almost as fast as you can think them up!

. . .



## `duckdb` adds more options

Note that it _also_ opens up additional functions via `dbplyr` that may not be added yet into `arrow`'s conversion layer.

:::: {.columns}

::: {.column width="45%"}


:::

::::

## `duckdb` adds more options

Note that it _also_ opens up additional functions via `dbplyr` that may not be added yet into `arrow`'s conversion layer.

:::: {.columns}

::: {.column width="45%"}



:::


::: {.column width="45%"}



:::

::::

## All together now

`dplyr`, `arrow`, and `duckdb` form a powerful trifecta for efficiently and effectively exploring large datasets.

. . .

Once you have explored and want to bring it into memory, it's also fast!

. . .



. . .



## We can `bench::mark()`



All are pretty fast!



. . .

But again - the beauty of `arrow` is not just that it's fast!

. . .

It's fast at exploring the data _BEFORE_ even having to wait for long reads OR having to get a workstation with enough memory to read it all in and compute on it!

. . .

So go out and use `arrow` + `dplyr` with `duckdb` for outrageously efficient exploratory data analysis!

## {background-image="howard.jpg" background-size="contain"}